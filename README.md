# MLOps Assignment 1: Machine Learning Pipeline with MLflow

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![MLflow](https://img.shields.io/badge/MLflow-2.0%2B-orange)](https://mlflow.org/)
[![scikit-learn](https://img.shields.io/badge/scikit--learn-1.0%2B-green)](https://scikit-learn.org/)

A comprehensive MLOps pipeline implementing model training, experiment tracking, monitoring, and model registry using MLflow.

## ğŸš€ Project Overview

This project demonstrates a complete MLOps workflow for machine learning model development and deployment, featuring:

- **Model Training**: Multiple ML algorithms with performance comparison
- **Experiment Tracking**: Comprehensive MLflow integration for reproducibility
- **Model Registry**: Centralized model versioning and lifecycle management
- **Performance Monitoring**: Automated best model selection and production tagging

## ğŸ“‚ Project Structure

```
mlops-assignment-1/
â”œâ”€â”€ ğŸ“„ README.md                    # Project documentation
â”œâ”€â”€ ğŸ“Š data/                        
â”‚   â””â”€â”€ iris.csv                   
â”œâ”€â”€ ğŸ¤– models/                      # Trained model artifacts
â”‚   â”œâ”€â”€ logistic_regression.joblib
â”‚   â”œâ”€â”€ random_forest.joblib
â”‚   â””â”€â”€ svm.joblib
â”œâ”€â”€ ğŸ““ notebooks/                   # Jupyter notebooks
â”‚   â””â”€â”€ model_training_comparison.ipynb
â”œâ”€â”€ ğŸ src/                         # Source code
â”‚   â”œâ”€â”€ mlflow_tracking.py         # MLflow experiment tracking
â”‚   â”œâ”€â”€ model_monitoring.py        # Model performance monitoring
â”‚   â””â”€â”€ model_registry_status.py   # Registry status utilities
â””â”€â”€ ğŸ“ˆ mlruns/                      # MLflow backend storage
```

## ğŸ”§ Prerequisites

- Python 3.8 or higher
- pip package manager

## ğŸ“¦ Installation

1. **Clone the repository**:
   ```bash
   git clone https://github.com/qasimzubair/MLOPS_A01.git
   cd mlops-assignment-1
   ```

2. **Install dependencies**:
   ```bash
   pip install mlflow scikit-learn pandas matplotlib seaborn jupyter
   ```

## ğŸƒâ€â™‚ï¸ Quick Start

### 1. Train Models and Track Experiments
```bash
python src/mlflow_tracking.py
```
This will:
- Load/prepare the Iris dataset
- Train three ML models (Logistic Regression, Random Forest, SVM)
- Log experiments to MLflow with metrics and artifacts
- Register models in the MLflow Model Registry

### 2. Monitor Model Performance
```bash
python src/model_monitoring.py
```
This will:
- Analyze all experiment runs
- Identify the best performing model
- Tag the best model for production deployment

### 3. Check Registry Status
```bash
python src/model_registry_status.py
```
View registered models and their current status.

### 4. Launch MLflow UI
```bash
mlflow ui
```
Access the web interface at `http://localhost:5000` to:
- Compare experiment runs
- View model performance metrics
- Manage model registry
- Analyze artifacts and visualizations

## ğŸ¤– Machine Learning Models

| Model | Algorithm | Accuracy | Status |
|-------|-----------|----------|--------|
| **Logistic Regression** | Linear Classification | 100% | Registered |
| **Random Forest** | Ensemble Method | 100% | Registered |
| **SVM** | Support Vector Machine | 100% | ğŸ† **Production** |

> **Note**: All models achieve perfect accuracy on the Iris dataset due to its simplicity and clean separation of classes.

## ğŸ“Š MLflow Features Implemented

### Experiment Tracking
- âœ… Parameter logging (hyperparameters)
- âœ… Metric logging (accuracy, precision, recall, F1-score)
- âœ… Artifact logging (confusion matrices, model files)
- âœ… Custom tags and metadata

### Model Registry
- âœ… Model versioning and lifecycle management
- âœ… Production model tagging
- âœ… Model comparison and selection
- âœ… Centralized model storage

### Monitoring & Governance
- âœ… Performance tracking across experiments
- âœ… Automated best model identification
- âœ… Production readiness assessment

## ğŸ“ˆ Performance Metrics

All models are evaluated using standard classification metrics:

- **Accuracy**: Overall classification correctness
- **Precision**: True positive rate (macro-averaged)
- **Recall**: Sensitivity (macro-averaged)  
- **F1-Score**: Harmonic mean of precision and recall
- **Confusion Matrix**: Detailed prediction breakdown

## ğŸ” MLflow UI Navigation

1. **Experiments Tab**: View all training runs with sortable metrics
2. **Models Tab**: Browse registered models and versions
3. **Compare Runs**: Select multiple experiments for side-by-side comparison
4. **Artifacts**: Download confusion matrices and model files

## ğŸš€ Production Deployment

The best performing model (SVM) is automatically tagged for production with:
- `deployment_status: production`
- `model_performance: best_model`
- `accuracy: 1.0`

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ‘¨â€ğŸ’» Author

**Qasim Zubair**
- GitHub: [@qasimzubair](https://github.com/qasimzubair)

## ğŸ™ Acknowledgments

- [MLflow](https://mlflow.org/) for experiment tracking and model management
- [scikit-learn](https://scikit-learn.org/) for machine learning algorithms
- [Iris Dataset](https://archive.ics.uci.edu/ml/datasets/iris) for the classic ML dataset

